import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from ripser import ripser
from persim import PersistenceImager

# ---- 1) Generate 30,000 random noisy datasets ----
num_datasets = 30000
num_points = 50
dim = 10
datasets = [np.random.randn(num_points, dim).astype(np.float32)
            for _ in range(num_datasets)]

# ---- 2) Compute Rips complexes and extract H1 diagrams ----
diagrams = []
for X in datasets:
    result = ripser(X, maxdim=1)
    dgm_h1 = result['dgms'][1]
    diagrams.append(dgm_h1)

# ---- 3) Build persistence images ----
imager = PersistenceImager(pixel_size=0.1)
imager.fit(diagrams, skew=True)

base_PIs = np.stack([imager.transform(dgm, skew=True).astype(np.float32)
                     for dgm in diagrams])
base_PIs = base_PIs.reshape(num_datasets, -1)

# ---- 4) Set up anchors ----
dim_pi = base_PIs.shape[1]
num_anchors = 10
anchors = torch.zeros(num_anchors, dim_pi)
for i in range(num_anchors):
    anchors[i, i % dim_pi] = 20.0

# ---- 5) Dataset and DataLoader ----
class PIPersistenceDataset(Dataset):
    def __init__(self, base_pis):
        self.pis = base_pis
    def __len__(self):
        return len(self.pis)
    def __getitem__(self, idx):
        return self.pis[idx]

dataset = PIPersistenceDataset(torch.from_numpy(base_PIs))
train_set, val_set, test_set = torch.utils.data.random_split(
    dataset, [24000, 3000, 3000],
    generator=torch.Generator().manual_seed(42))
batch_size = 32
train_loader = DataLoader(train_set, batch_size, shuffle=True)
val_loader   = DataLoader(val_set, batch_size)
test_loader  = DataLoader(test_set, batch_size)

# ---- 6) AlphaNet ----
class AlphaNet(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(AlphaNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, output_dim),
            nn.Softplus()
        )
    def forward(self, pi):
        return self.net(pi)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = AlphaNet(input_dim=dim_pi, output_dim=dim_pi).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# ---- 7) Softmax Loss with Diversity ----
def compute_loss(weighted_pi, anchors, counts, D, epoch):
    dists = torch.cdist(weighted_pi, anchors, p=2)
    temperature = 0.1
    sim = -dists / temperature
    probs = torch.softmax(sim, dim=1)
    soft_loss = (probs * dists.pow(2)).sum(dim=1).mean()

    p_hat = probs.sum(dim=0) / D  # Empirical distribution
    target = torch.full_like(p_hat, 1.0 / len(anchors))

    diversity_loss = D * torch.sum((p_hat - target)**2)

    return 0.5 * soft_loss + 0.5 * diversity_loss, p_hat

# ---- 8) Training ----
num_epochs = 100
running_counts = torch.zeros(num_anchors, device=device)
D_total = 0

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    D = 0
    running_counts.zero_()

    for batch in train_loader:
        pis = batch.to(device)
        alphas = model(pis)
        weighted = pis * alphas

        loss, p_hat = compute_loss(weighted, anchors.to(device), running_counts, D_total + 1, epoch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * pis.size(0)
        D += pis.size(0)
        running_counts += p_hat * pis.size(0)
    
    D_total += D
    epoch_loss /= len(train_loader.dataset)

    # Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for batch in val_loader:
            pis = batch.to(device)
            alphas = model(pis)
            weighted = pis * alphas
            loss, _ = compute_loss(weighted, anchors.to(device), running_counts, D_total + 1, epoch)
            val_loss += loss.item() * pis.size(0)
    val_loss /= len(val_loader.dataset)

    print(f"Epoch {epoch+1}/{num_epochs}: Train Loss = {epoch_loss:.4f}, Val Loss = {val_loss:.4f}")

# ---- 9) Test ----
model.eval()
test_loss = 0.0
with torch.no_grad():
    for batch in test_loader:
        pis = batch.to(device)
        alphas = model(pis)
        weighted = pis * alphas
        loss, _ = compute_loss(weighted, anchors.to(device), running_counts, D_total + 1, epoch)
        test_loss += loss.item() * pis.size(0)
test_loss /= len(test_loader.dataset)
print(f"Test Loss = {test_loss:.4f}")
